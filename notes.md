

### Novel thermodynamic properties of computational processes (David Wolpert and Artemy Kolchinsky)

5% of US expenditure.
Allowing noise in computation reduces energy requirements. Increase errors at the chip level,
basically transfer noise from the environment into the processing.

"Intelligence is extremely costly. Evolution will try to keep people stupid. There is a big
evolutionary advantage to being Donald Trump."

The biosphere's computational efficiency is orders of magnitude better than the best computer.

Dissipation depends on the mismatch between the assumed priors in the computational design and the
actual requirements of the tasks being carried out.

There is a thermodynamic version of Kolmogorov complexity, which is that you replace the minimal
size of a string that produces a given output with the minimal *work* required to produce that
same bitstring. Minimal work is bounded even though Kolmogorov complexity is not (the expected work
is infinite).

Lots of work to be done on "thermodynamicising" computer science, see for instance if you get the
same complexity classes and the such.

  * => Check out circuit design theory, has interesting implications with complex problems. Might
    have interesting implications for communication over graphs. Random graphs creating circuits
    with varying properties, notably in the difference between expectations and actual distribution
    of events.
    - I wonder if this could be in any way related to expectations in Bayesian systems, and how to
      bridge them to QI and imaginary amplitudes
    - check out "information reservoirs". Do they produce waves by "emptying" regularly (possibly
      at criticality)

### Entropy production for coarse-grained dynamics (Daniel Maria Busiello, Jorge Hidalgo Aguilera, and Amos Maritan)
